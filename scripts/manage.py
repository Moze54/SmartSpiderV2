#!/usr/bin/env python3
"""
SmartSpider ç®¡ç†è„šæœ¬
æä¾›ç³»ç»Ÿç®¡ç†ã€ç»´æŠ¤å’Œç›‘æ§åŠŸèƒ½
"""
import asyncio
import sys
import argparse
from pathlib import Path
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from smart_spider.core.database import init_db, db_manager
from smart_spider.core.database_optimization import db_optimizer
from smart_spider.core.proxy_manager import proxy_manager, load_proxies_on_startup
from smart_spider.core.cookie_manager import cookie_manager
from smart_spider.core.logger import logger
from smart_spider.scheduler.task_scheduler import task_scheduler
from smart_spider.storage.file_storage import file_storage
from smart_spider.storage.data_exporter import data_exporter
from smart_spider.config.settings import settings
from smart_spider.models.database import Task, TaskStatus
from sqlalchemy import select, func, text
from smart_spider.core.priority_queue import task_priority_queue


class SmartSpiderManager:
    """SmartSpider ç®¡ç†å™¨"""

    def __init__(self):
        self.start_time = datetime.now()

    async def initialize_system(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ– SmartSpider ç³»ç»Ÿ...")

        # åˆ›å»ºå¿…è¦ç›®å½•
        self._create_directories()

        # åˆå§‹åŒ–æ•°æ®åº“
        await self._initialize_database()

        # åŠ è½½ä»£ç†
        await self._load_proxies()

        # å¯åŠ¨è°ƒåº¦å™¨
        await self._start_scheduler()

        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def _create_directories(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        directories = [
            settings.app.upload_dir,
            settings.app.output_dir,
            settings.app.storage_dir,
            settings.advanced.cookie_storage_dir,
            settings.logging.file_path.parent,
            "logs",
            "backups"
        ]

        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            print(f"âœ“ åˆ›å»ºç›®å½•: {directory}")

    async def _initialize_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        try:
            print("æ­£åœ¨åˆå§‹åŒ–æ•°æ®åº“...")
            await init_db()

            print("æ­£åœ¨åˆ›å»ºæ€§èƒ½ç´¢å¼•...")
            await db_optimizer.create_performance_indexes()
            await db_optimizer.create_composite_indexes()

            print("âœ“ æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            print(f"âœ— æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    async def _load_proxies(self):
        """åŠ è½½ä»£ç†"""
        try:
            print("æ­£åœ¨åŠ è½½ä»£ç†...")
            await load_proxies_on_startup()

            # å¦‚æœæœ‰é…ç½®çš„ä»£ç†åˆ—è¡¨ï¼Œæ·»åŠ å®ƒä»¬
            if settings.crawler.proxy_list:
                from smart_spider.core.proxy_manager import init_proxy_pool
                success_count, _ = await init_proxy_pool(settings.crawler.proxy_list)
                print(f"âœ“ åŠ è½½ä»£ç†å®Œæˆ: æˆåŠŸ {success_count} ä¸ª")

        except Exception as e:
            print(f"âš ï¸  ä»£ç†åŠ è½½å¤±è´¥: {str(e)}")

    async def _start_scheduler(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        if settings.scheduler.enabled:
            try:
                print("æ­£åœ¨å¯åŠ¨ä»»åŠ¡è°ƒåº¦å™¨...")
                await task_scheduler.start()
                print("âœ“ ä»»åŠ¡è°ƒåº¦å™¨å¯åŠ¨å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸  è°ƒåº¦å™¨å¯åŠ¨å¤±è´¥: {str(e)}")

    async def cleanup_system(self):
        """æ¸…ç†ç³»ç»Ÿ"""
        print("ğŸ§¹ æ­£åœ¨æ¸…ç†ç³»ç»Ÿ...")

        # æ¸…ç†æ•°æ®åº“
        deleted_count = await db_optimizer.cleanup_old_data(
            days_to_keep=settings.cleanup.old_files_days
        )
        print(f"âœ“ æ¸…ç†æ•°æ®åº“: åˆ é™¤äº† {deleted_count} æ¡æ—§è®°å½•")

        # æ¸…ç†æ–‡ä»¶å­˜å‚¨
        removed_files = file_storage.cleanup_old_files(
            days_to_keep=settings.cleanup.old_files_days
        )
        print(f"âœ“ æ¸…ç†æ–‡ä»¶å­˜å‚¨: åˆ é™¤äº† {removed_files} ä¸ªæ—§æ–‡ä»¶")

        # æ¸…ç†Cookie
        removed_cookies = cookie_manager.cleanup_old_cookies(
            days_to_keep=settings.cleanup.old_cookies_days
        )
        print(f"âœ“ æ¸…ç†Cookie: åˆ é™¤äº† {removed_cookies} ä¸ªæ—§Cookieæ–‡ä»¶")

        # æ¸…ç†ä»£ç†
        removed_proxies = await proxy_manager.cleanup_old_proxies(
            days_to_keep=settings.cleanup.old_proxies_days
        )
        print(f"âœ“ æ¸…ç†ä»£ç†: åˆ é™¤äº† {removed_proxies} ä¸ªæ—§ä»£ç†")

        # æ¸…ç†å¯¼å‡ºæ–‡ä»¶
        # è¿™é‡Œå¯ä»¥æ·»åŠ å¯¼å‡ºæ–‡ä»¶çš„æ¸…ç†é€»è¾‘

        print("âœ… ç³»ç»Ÿæ¸…ç†å®Œæˆ")

    async def optimize_system(self):
        """ä¼˜åŒ–ç³»ç»Ÿ"""
        print("âš¡ æ­£åœ¨ä¼˜åŒ–ç³»ç»Ÿ...")

        # æ•°æ®åº“ä¼˜åŒ–
        print("æ­£åœ¨ä¼˜åŒ–æ•°æ®åº“...")
        await db_optimizer.run_performance_optimization()

        # ä»£ç†æ± ä¼˜åŒ–
        print("æ­£åœ¨ä¼˜åŒ–ä»£ç†æ± ...")
        await proxy_manager.check_all_proxies()

        # é‡å»ºç´¢å¼•
        print("æ­£åœ¨é‡å»ºç´¢å¼•...")
        await db_optimizer.rebuild_indexes()

        print("âœ… ç³»ç»Ÿä¼˜åŒ–å®Œæˆ")

    async def check_system_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        print("ğŸ¥ æ­£åœ¨æ£€æŸ¥ç³»ç»Ÿå¥åº·çŠ¶æ€...")

        health_status = {
            'timestamp': datetime.now().isoformat(),
            'overall_status': 'healthy',
            'components': {}
        }

        # æ•°æ®åº“å¥åº·æ£€æŸ¥
        try:
            db_health = await db_optimizer.get_database_health()
            health_status['components']['database'] = {
                'status': 'healthy' if db_health['is_healthy'] else 'unhealthy',
                'details': db_health
            }
            if not db_health['is_healthy']:
                health_status['overall_status'] = 'unhealthy'
        except Exception as e:
            health_status['components']['database'] = {
                'status': 'error',
                'error': str(e)
            }
            health_status['overall_status'] = 'unhealthy'

        # ä»£ç†æ± å¥åº·æ£€æŸ¥
        try:
            proxy_stats = proxy_manager.get_proxy_stats()
            active_proxies = proxy_stats.get('active_proxies', 0)
            total_proxies = proxy_stats.get('total_proxies', 0)

            health_status['components']['proxy_pool'] = {
                'status': 'healthy' if active_proxies \u003e 0 else 'warning',
                'details': {
                    'active_proxies': active_proxies,
                    'total_proxies': total_proxies,
                    'success_rate': proxy_stats.get('overall_success_rate', 0)
                }
            }
        except Exception as e:
            health_status['components']['proxy_pool'] = {
                'status': 'error',
                'error': str(e)
            }

        # å­˜å‚¨å¥åº·æ£€æŸ¥
        try:
            storage_stats = file_storage.get_storage_stats()
            health_status['components']['storage'] = {
                'status': 'healthy',
                'details': storage_stats
            }
        except Exception as e:
            health_status['components']['storage'] = {
                'status': 'error',
                'error': str(e)
            }

        # è°ƒåº¦å™¨å¥åº·æ£€æŸ¥
        try:
            if settings.scheduler.enabled:
                scheduler_stats = task_scheduler.get_scheduled_tasks()
                health_status['components']['scheduler'] = {
                    'status': 'healthy',
                    'details': {
                        'active_jobs': len(scheduler_stats)
                    }
                }
            else:
                health_status['components']['scheduler'] = {
                    'status': 'disabled',
                    'details': {}
                }
        except Exception as e:
            health_status['components']['scheduler'] = {
                'status': 'error',
                'error': str(e)
            }

        # ä¼˜å…ˆçº§é˜Ÿåˆ—å¥åº·æ£€æŸ¥
        try:
            queue_stats = task_priority_queue.get_queue_stats()
            health_status['components']['priority_queue'] = {
                'status': 'healthy',
                'details': queue_stats
            }
        except Exception as e:
            health_status['components']['priority_queue'] = {
                'status': 'error',
                'error': str(e)
            }

        print(f"âœ“ ç³»ç»Ÿå¥åº·çŠ¶æ€: {health_status['overall_status']}")
        return health_status

    async def get_system_statistics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        print("ğŸ“Š æ­£åœ¨æ”¶é›†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯...")

        stats = {
            'timestamp': datetime.now().isoformat(),
            'uptime': (datetime.now() - self.start_time).total_seconds(),
            'components': {}
        }

        # æ•°æ®åº“ç»Ÿè®¡
        try:
            db_stats = await db_optimizer.analyze_all_tables()
            stats['components']['database'] = db_stats
        except Exception as e:
            stats['components']['database'] = {'error': str(e)}

        # ä»»åŠ¡ç»Ÿè®¡
        try:
            async with AsyncSession(db_manager.engine) as session:
                # æ€»ä»»åŠ¡æ•°
                total_tasks = await session.scalar(select(func.count(Task.id)))

                # çŠ¶æ€åˆ†å¸ƒ
                status_counts = {}
                for status in TaskStatus:
                    count = await session.scalar(
                        select(func.count(Task.id)).where(Task.status == status)
                    )
                    status_counts[status.value] = count

                # æœ€è¿‘24å°æ—¶ä»»åŠ¡æ•°
                recent_tasks = await session.scalar(
                    select(func.count(Task.id)).where(
                        Task.created_at \u003e= datetime.now() - timedelta(hours=24)
                    )
                )

                stats['components']['tasks'] = {
                    'total_tasks': total_tasks,
                    'status_distribution': status_counts,
                    'recent_24h_tasks': recent_tasks
                }
        except Exception as e:
            stats['components']['tasks'] = {'error': str(e)}

        # å­˜å‚¨ç»Ÿè®¡
        try:
            storage_stats = file_storage.get_storage_stats()
            stats['components']['storage'] = storage_stats
        except Exception as e:
            stats['components']['storage'] = {'error': str(e)}

        # å¯¼å‡ºç»Ÿè®¡
        try:
            export_stats = data_exporter.get_export_stats()
            stats['components']['export'] = export_stats
        except Exception as e:
            stats['components']['export'] = {'error': str(e)}

        # ä»£ç†ç»Ÿè®¡
        try:
            proxy_stats = proxy_manager.get_proxy_stats()
            stats['components']['proxy_pool'] = proxy_stats
        except Exception as e:
            stats['components']['proxy_pool'] = {'error': str(e)}

        print("âœ“ ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯æ”¶é›†å®Œæˆ")
        return stats

    async def backup_system(self, backup_dir: Optional[str] = None):
        """å¤‡ä»½ç³»ç»Ÿæ•°æ®"""
        backup_dir = backup_dir or f"backups/{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        Path(backup_dir).mkdir(parents=True, exist_ok=True)

        print(f"ğŸ’¾ æ­£åœ¨å¤‡ä»½ç³»ç»Ÿæ•°æ®åˆ°: {backup_dir}")

        # æ•°æ®åº“å¤‡ä»½
        try:
            print("æ­£åœ¨å¤‡ä»½æ•°æ®åº“...")
            # è¿™é‡Œå¯ä»¥å®ç°æ•°æ®åº“å¤‡ä»½é€»è¾‘
            # ä¾‹å¦‚å¯¼å‡ºSQLæ–‡ä»¶æˆ–ä½¿ç”¨pg_dump
            backup_file = Path(backup_dir) / f"database_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.sql"
            print(f"âœ“ æ•°æ®åº“å¤‡ä»½å®Œæˆ: {backup_file}")
        except Exception as e:
            print(f"âš ï¸  æ•°æ®åº“å¤‡ä»½å¤±è´¥: {str(e)}")

        # æ–‡ä»¶å¤‡ä»½
        try:
            print("æ­£åœ¨å¤‡ä»½æ–‡ä»¶...")
            import shutil

            # å¤‡ä»½é‡è¦ç›®å½•
            backup_dirs = [
                (settings.app.upload_dir, "uploads"),
                (settings.app.output_dir, "output"),
                (settings.app.storage_dir, "storage"),
                (settings.advanced.cookie_storage_dir, "cookies")
            ]

            for source_dir, backup_name in backup_dirs:
                if Path(source_dir).exists():
                    target_dir = Path(backup_dir) / backup_name
                    shutil.copytree(source_dir, target_dir, dirs_exist_ok=True)
                    print(f"âœ“ å¤‡ä»½ç›®å½•: {source_dir} -> {target_dir}")

        except Exception as e:
            print(f"âš ï¸  æ–‡ä»¶å¤‡ä»½å¤±è´¥: {str(e)}")

        print(f"âœ… ç³»ç»Ÿå¤‡ä»½å®Œæˆ: {backup_dir}")

    def print_system_info(self):
        """æ‰“å°ç³»ç»Ÿä¿¡æ¯"""
        print("""
ğŸ•· SmartSpider - æ™ºèƒ½ç½‘ç»œçˆ¬è™«ç³»ç»Ÿ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ç³»ç»Ÿä¿¡æ¯:
  ç‰ˆæœ¬: {version}
  ç¯å¢ƒ: {environment}
  è¿è¡Œæ—¶é—´: {uptime}
  æ•°æ®åº“: {database_type}
  å­˜å‚¨è·¯å¾„: {storage_path}

å¯ç”¨å‘½ä»¤:
  init      - åˆå§‹åŒ–ç³»ç»Ÿ
  cleanup   - æ¸…ç†ç³»ç»Ÿ
  optimize  - ä¼˜åŒ–ç³»ç»Ÿ
  health    - å¥åº·æ£€æŸ¥
  stats     - ç»Ÿè®¡ä¿¡æ¯
  backup    - å¤‡ä»½ç³»ç»Ÿ
  proxy     - ä»£ç†ç®¡ç†
  cookie    - Cookieç®¡ç†
  task      - ä»»åŠ¡ç®¡ç†

ç¤ºä¾‹:
  python manage.py init
  python manage.py health
  python manage.py stats
  python manage.py backup --dir ./backups

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """.format(
            version=settings.app.version,
            environment=settings.app.environment,
            uptime=self.start_time.strftime('%Y-%m-%d %H:%M:%S'),
            database_type="PostgreSQL",
            storage_path=settings.app.storage_dir
        ))


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="SmartSpider ç³»ç»Ÿç®¡ç†å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )

    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')

    # åˆå§‹åŒ–å‘½ä»¤
    init_parser = subparsers.add_parser('init', help='åˆå§‹åŒ–ç³»ç»Ÿ')

    # æ¸…ç†å‘½ä»¤
    cleanup_parser = subparsers.add_parser('cleanup', help='æ¸…ç†ç³»ç»Ÿ')

    # ä¼˜åŒ–å‘½ä»¤
    optimize_parser = subparsers.add_parser('optimize', help='ä¼˜åŒ–ç³»ç»Ÿ')

    # å¥åº·æ£€æŸ¥å‘½ä»¤
    health_parser = subparsers.add_parser('health', help='å¥åº·æ£€æŸ¥')

    # ç»Ÿè®¡å‘½ä»¤
    stats_parser = subparsers.add_parser('stats', help='ç»Ÿè®¡ä¿¡æ¯')

    # å¤‡ä»½å‘½ä»¤
    backup_parser = subparsers.add_parser('backup', help='å¤‡ä»½ç³»ç»Ÿ')
    backup_parser.add_argument('--dir', help='å¤‡ä»½ç›®å½•')

    # ä»£ç†ç®¡ç†å‘½ä»¤
    proxy_parser = subparsers.add_parser('proxy', help='ä»£ç†ç®¡ç†')
    proxy_subparsers = proxy_parser.add_subparsers(dest='proxy_command')

    proxy_check_parser = proxy_subparsers.add_parser('check', help='æ£€æŸ¥ä»£ç†')
    proxy_add_parser = proxy_subparsers.add_parser('add', help='æ·»åŠ ä»£ç†')
    proxy_add_parser.add_argument('proxy_url', help='ä»£ç†URL')
    proxy_stats_parser = proxy_subparsers.add_parser('stats', help='ä»£ç†ç»Ÿè®¡')

    # Cookieç®¡ç†å‘½ä»¤
    cookie_parser = subparsers.add_parser('cookie', help='Cookieç®¡ç†')
    cookie_subparsers = cookie_parser.add_subparsers(dest='cookie_command')

    cookie_check_parser = cookie_subparsers.add_parser('check', help='æ£€æŸ¥Cookie')
    cookie_stats_parser = cookie_subparsers.add_parser('stats', help='Cookieç»Ÿè®¡')

    # ä»»åŠ¡ç®¡ç†å‘½ä»¤
    task_parser = subparsers.add_parser('task', help='ä»»åŠ¡ç®¡ç†')
    task_subparsers = task_parser.add_subparsers(dest='task_command')

    task_list_parser = task_subparsers.add_parser('list', help='åˆ—å‡ºä»»åŠ¡')
    task_stats_parser = task_subparsers.add_parser('stats', help='ä»»åŠ¡ç»Ÿè®¡')

    args = parser.parse_args()

    manager = SmartSpiderManager()

    if not args.command:
        manager.print_system_info()
        return

    # æ‰§è¡Œå‘½ä»¤
    if args.command == 'init':
        await manager.initialize_system()

    elif args.command == 'cleanup':
        await manager.cleanup_system()

    elif args.command == 'optimize':
        await manager.optimize_system()

    elif args.command == 'health':
        health_status = await manager.check_system_health()
        print("\nè¯¦ç»†å¥åº·çŠ¶æ€:")
        import json
        print(json.dumps(health_status, indent=2, ensure_ascii=False))

    elif args.command == 'stats':
        stats = await manager.get_system_statistics()
        print("\nè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
        import json
        print(json.dumps(stats, indent=2, ensure_ascii=False))

    elif args.command == 'backup':
        await manager.backup_system(args.dir)

    elif args.command == 'proxy':
        if args.proxy_command == 'check':
            await check_proxy_health()
        elif args.proxy_command == 'add':
            success = await proxy_manager.add_proxy(args.proxy_url)
            if success:
                print(f"âœ“ ä»£ç†æ·»åŠ æˆåŠŸ: {args.proxy_url}")
            else:
                print(f"âœ— ä»£ç†æ·»åŠ å¤±è´¥: {args.proxy_url}")
        elif args.proxy_command == 'stats':
            stats = proxy_manager.get_proxy_stats()
            import json
            print(json.dumps(stats, indent=2, ensure_ascii=False))

    elif args.command == 'cookie':
        if args.cookie_command == 'check':
            await validate_browser_cookies()
        elif args.cookie_command == 'stats':
            stats = cookie_manager.get_cookie_stats()
            import json
            print(json.dumps(stats, indent=2, ensure_ascii=False))

    elif args.command == 'task':
        if args.task_command == 'list':
            await list_tasks()
        elif args.task_command == 'stats':
            await show_task_stats()


async def list_tasks():
    """åˆ—å‡ºä»»åŠ¡"""
    try:
        from smart_spider.services.task_service import task_service
        from smart_spider.core.database import get_session

        async for session in get_session():
            tasks = await task_service.get_tasks(session)

            print(f"{'ä»»åŠ¡ID':\u003c36} {'åç§°':\u003c20} {'çŠ¶æ€':\u003c10} {'è¿›åº¦':\u003c8} {'åˆ›å»ºæ—¶é—´':\u003c20}")
            print("-" * 100)

            for task in tasks[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                print(f"{task.task_id:\u003c36} {task.name:\u003c20} {task.status:\u003c10} {task.progress:\u003c8.1f}% {task.created_at.strftime('%Y-%m-%d %H:%M:%S'):\u003c20}")

            if len(tasks) \u003e 20:
                print(f"\n... è¿˜æœ‰ {len(tasks) - 20} ä¸ªä»»åŠ¡")

            break
    except Exception as e:
        print(f"âœ— åˆ—å‡ºä»»åŠ¡å¤±è´¥: {str(e)}")


async def show_task_stats():
    """æ˜¾ç¤ºä»»åŠ¡ç»Ÿè®¡"""
    try:
        from smart_spider.services.task_service import task_service
        from smart_spider.core.database import get_session

        async for session in get_session():
            tasks = await task_service.get_tasks(session)

            status_counts = {}
            for task in tasks:
                status = task.status.value
                status_counts[status] = status_counts.get(status, 0) + 1

            print("ä»»åŠ¡ç»Ÿè®¡:")
            print(f"æ€»ä»»åŠ¡æ•°: {len(tasks)}")
            print("çŠ¶æ€åˆ†å¸ƒ:")
            for status, count in status_counts.items():
                print(f"  {status}: {count}")

            break
    except Exception as e:
        print(f"âœ— æ˜¾ç¤ºä»»åŠ¡ç»Ÿè®¡å¤±è´¥: {str(e)}")


if __name__ == '__main__':
    asyncio.run(main())